{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kennedy/opt/anaconda3/envs/emotion-detection/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Standard Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import sys\n",
    "from datetime import date\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "\n",
    "\n",
    "# Custom Libraries\n",
    "from file_proc import *\n",
    "#import database\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Image Libraries\n",
    "import cv2\n",
    "from cv2 import imread, imshow, waitKey, destroyAllWindows, rectangle, CascadeClassifier\n",
    "\n",
    "# Database\n",
    "import psycopg2\n",
    "\n",
    "# Visualizations\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  908k  100  908k    0     0   687k      0  0:00:01  0:00:01 --:--:--  687k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  184k  100  184k    0     0   373k      0 --:--:-- --:--:-- --:--:--  372k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  333k  100  333k    0     0   648k      0 --:--:-- --:--:-- --:--:--  649k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  587k  100  587k    0     0   546k      0  0:00:01  0:00:01 --:--:--  546k\n"
     ]
    }
   ],
   "source": [
    "# Get the classifier .xml files and save them in the src directory\n",
    "! curl -o ../src/haarcascade_frontalface_default.xml https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "! curl -o ../src/haarcascade_smile.xml https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_smile.xml\n",
    "! curl -o ../src/haarcascade_eye.xml https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml\n",
    "! curl -o ../src/haarcascade_eye_tree_eyeglasses.xml https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye_tree_eyeglasses.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '' # image name\n",
    "file_path = '' # path to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560 ../data/TD_RGB_E/TD_RGB_E_Set4/95/TD_RGB_E_5.jpg\n"
     ]
    }
   ],
   "source": [
    "# Process Images Directory for Feeding into the Model\n",
    "image_list = build_image_list('../data/TD_RGB_E')\n",
    "print(len(image_list), image_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNUSED BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, box_list):\n",
    "    '''\n",
    "    draw boxes detected in box list.  Also calculate region of image (roi) to be passed to another function.\n",
    "    INPUTS: image : image to be modified. \n",
    "            box_list : list of box coordinates.\n",
    "    OUTPUTS: annotated image, roi\n",
    "    '''\n",
    "    for box in box_list:\n",
    "        x, y, w, h = box\n",
    "        x2, y2 = x + w, y + h\n",
    "        cv2.rectangle(image, (x, y), (x2, y2), (0,255,0), 3)\n",
    "        roi = image[y:y2, x:x2]\n",
    "    #return save_image(image, roi)\n",
    "    return image, roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img_rgb, roi):\n",
    "#     plt.imshow(img)\n",
    "#     plt.xticks([]), plt.yticks([])\n",
    "#     plt.show()\n",
    "    img_rgb = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2RGB)\n",
    "    # write to db cv2.imwrite('../sort/face/'+file_name+'.jpg', img_rgb)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    img_rgb = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2RGB)\n",
    "    # write to db cv2.imwrite('../sort/face/'+file_name+'.jpg', img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(classifier, id_, con):\n",
    "    '''\n",
    "    HAAR classifier for face detection. iI faces detected: update feature from False to True, draw box around face, write image to new field in database\n",
    "    read image from original column\n",
    "    INPUTS: classifier, image file from database, database column to write. emotions = [ 0,1,2,3,4,5] # 0=no_class, 1=commecicomm,2=smile,3=shut,4=shocked,5=sunglasses\n",
    "    OUTPUTS: Annotated image saved to database, database flag updated.\n",
    "    '''\n",
    "    classifier_fishface = cv2.createFisherFaceRecognizer() #Initialize fisher face classifier\n",
    "    image = cv2.imread(imagePath) # SELECT img_orig from images WHERE id == id_\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=7, minSize=(50, 50))\n",
    "    print('Found {} faces!'.format(len(faces)))\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "    # write img to database\n",
    "    # update flagired\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection with VGG16 Pretrained CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the data into labels (encoded) and images converted to a 224x224x3 numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['commecicomm','happy','ugh','shocked','sunglasses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageDataGenerator()\n",
    "traindata = train.flow_from_directory(directory='..data/train',target_size=(224,224))\n",
    "test = ImageDataGenerator()\n",
    "testdata = test.flow_from_directory(directory='..data/test', target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=5)\n",
    "\n",
    "#model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "img_path = 'elephant.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = model.predict(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1db474ace717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = Sequential([\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu',input_shape=(350,350,1)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d20c3f71d4ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# model.add(BatchNormalization())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model.add(BatchNormalization())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(350,350,1)))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(7, kernel_size=(1, 1), activation='relu'))\n",
    "# # model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(7, kernel_size=(4, 4), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(steps_per_epoch=100,generator=traindata, validation_data= testdata, validation_steps=10,epochs=100,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "img = image.load_img(\"image.jpeg\",target_size=(224,224))\n",
    "img = np.asarray(img)\n",
    "plt.imshow(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "from keras.models import load_model\n",
    "saved_model = load_model(\"vgg16_1.h5\")\n",
    "output = saved_model.predict(img)\n",
    "if output[0][0] > output[0][1]:\n",
    "    print(\"cat\")\n",
    "else:\n",
    "    print('dog')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image = load_img('mug.jpg', target_size=(224, 224))\n",
    "1\n",
    "2\n",
    "3\n",
    "from keras.preprocessing.image import load_img\n",
    "# load an image from file\n",
    "image = load_img('mug.jpg', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'elephant.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(steps_per_epoch=100,generator=traindata, validation_data= testdata, validation_steps=10,epochs=100,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "img = image.load_img(\"image.jpeg\",target_size=(224,224))\n",
    "img = np.asarray(img)\n",
    "plt.imshow(img)\n",
    "img = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "saved_model = load_model(\"vgg16_1.h5\")\n",
    "output = saved_model.predict(img)\n",
    "if output[0][0] > output[0][1]:\n",
    "    print(\"cat\")\n",
    "else:\n",
    "    print('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_database(db='kennedy', user_id='postgres', passwd=''):\n",
    "    \n",
    "    con = None\n",
    "\n",
    "    try:\n",
    "    \n",
    "        con = psycopg2.connect(database=db, user=user_id,\n",
    "            password=passwd)\n",
    "\n",
    "        cur = con.cursor()\n",
    "        cur.execute('SELECT version()')\n",
    "        version = cur.fetchone()[0]\n",
    "        print(version)\n",
    "\n",
    "    except psycopg2.DatabaseError as e:\n",
    "\n",
    "        print('Error {}'.format(e))\n",
    "        sys.exit(1)\n",
    "\n",
    "    finally:\n",
    "\n",
    "        if con:\n",
    "            con.close()\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_database(db='kennedy', user_id='postgres', passwd=''):\n",
    "    \n",
    "    con = None\n",
    "\n",
    "    try:\n",
    "    \n",
    "        con = psycopg2.connect(database=db, user=user_id,\n",
    "            password=passwd)\n",
    "        cur = con.cursor()\n",
    "        \n",
    "    except psycopg2.DatabaseError as e:\n",
    "        \n",
    "        print('Error {}'.format(e))\n",
    "        sys.exit(1)\n",
    "        \n",
    "    return con, cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_database(con):\n",
    "    \n",
    "    if con:\n",
    "        con.commit()\n",
    "        con.close()\n",
    "        \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(con, cur):\n",
    "\n",
    "        with con:\n",
    "\n",
    "            cur.execute(\"DROP TABLE IF EXISTS images\")\n",
    "            cur.execute(\"CREATE TABLE images(id BIGINT DEFAULT 0 NOT NULL, file_name TEXT, dir_path TEXT, img_orig BYTEA, date DATE DEFAULT '1/1/1970' NOT NULL, face BOOLEAN DEFAULT 'False' NOT NULL, face_class INTEGER DEFAULT 0 NOT NULL)\") # img_proc BYTEA, \n",
    "            cur.execute(\"COMMENT ON COLUMN images.id IS E'This number will come from a counter programatically.'\")\n",
    "            cur.execute(\"COMMENT ON COLUMN images.dir_path IS E'directory path to original image.  This is so that you can trace back to the imported data for files with the same names in multiple directories.'\")\n",
    "            cur.execute(\"COMMENT ON COLUMN images.date IS E'date image was imported.'\")\n",
    "            cur.execute(\"COMMENT ON COLUMN images.face_class IS E'Classes based off _# in filename.  0,1,2,3,4,5. 0=no_class, 1=commecicomm,2=smile,3=shut,4=shocked,5=sunglasses.'\")\n",
    "            cur.execute(\"ALTER TABLE images ADD CONSTRAINT dir_path PRIMARY KEY(id)\")\n",
    "            cur.execute(\"ALTER TABLE images ADD COLUMN class_pred INTEGER DEFAULT 0 NOT NULL\")\n",
    "            cur.execute(\"ALTER TABLE images ADD COLUMN ext TEXT DEFAULT '.jpg' NOT NULL\")\n",
    "            con.commit()\n",
    "            cur.execute(\"SELECT column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE table_name = 'images'\")\n",
    "            db_cols = cur.fetchall()\n",
    "            print(db_cols)\n",
    "        return 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_insert_database(con, cur, id_, file_name, dir_path):\n",
    "\n",
    "    with con:\n",
    "        \n",
    "        today = date.today().strftime('%m/%d/%Y')\n",
    "        imgopen = open(dir_path, 'rb').read()\n",
    "        face_class = file_name[-5]\n",
    "        data = (id_, dir_path, psycopg2.Binary(imgopen), today , False, face_class)\n",
    "        query = \"INSERT INTO images(id, dir_path, img_orig, date, face, face_class) VALUES (%s, %s, %s, %s, %s, %s)\" \n",
    "        cur.execute(query, data)\n",
    "        \n",
    "    return con, cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_database(fname):\n",
    "    file_name = '' # image name\n",
    "    file_path = '' # path to file\n",
    "    i = 0\n",
    "    for file in fname:\n",
    "        file_name = os.path.basename(file)\n",
    "        file_path = file\n",
    "        print('{}...{}...{}'.format(i, file_name, file_path))\n",
    "        bulk_insert_database(con, cur, i, file_name, file_path) \n",
    "        i+=1\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to take id and path as inputs.  id to select a specific row, path to where you wish to output (visualizations)\n",
    "def write_image_database_to_file(con, cur):\n",
    "    try:\n",
    "        \n",
    "        data = ('id', 'dir_path', 'date', 'img_orig' , 'face', 'face_class')\n",
    "        cur.execute(\"SELECT data FROM images\")\n",
    "        data = cur.fetchone()\n",
    "        open(os.path.expanduser('~/Desktop/test.jpp'), 'wb').write(data[3])        \n",
    "\n",
    "    except psycopg2.DatabaseError as e:\n",
    "\n",
    "        print('Error {}'.format(e))\n",
    "        sys.exit(9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_database(con):\n",
    "    query = \"SELECT id, dir_path, img_orig, face, face_class, class_pred FROM images\"\n",
    "    df_table = pd.read_sql_query(query, con)\n",
    "    return df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_image_database(con, cur, data):\n",
    "#     data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\n",
    "#     cv2.imwrite('../test/test_image.jpg', data)\n",
    "    return  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con, cur = connect_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_database(con, cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_images_database(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from database\n",
    "df = read_image_database(con)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
